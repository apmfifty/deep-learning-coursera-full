{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 21490.32it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('21 march 1973', '1973-03-21'),\n",
       " ('sunday july 8 2012', '2012-07-08'),\n",
       " ('thursday april 9 2015', '2015-04-09'),\n",
       " ('8/10/96', '1996-08-10'),\n",
       " ('monday january 22 1990', '1990-01-22'),\n",
       " ('wednesday may 5 1976', '1976-05-05'),\n",
       " ('05 jun 2017', '2017-06-05'),\n",
       " ('8 09 81', '1981-09-08'),\n",
       " ('friday december 17 2010', '2010-12-17'),\n",
       " ('thursday may 31 2001', '2001-05-31')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 21 march 1973\n",
      "Target date: 1973-03-21\n",
      "\n",
      "Source after preprocessing (indices): [ 5  4  0 24 13 28 15 20  0  4 12 10  6 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10  8  4  0  1  4  0  3  2]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator=RepeatVector(Tx)\n",
    "concatenator=Concatenate(axis=-1)\n",
    "densor1=Dense(10,activation='tanh')\n",
    "densor2=Dense(1,activation='relu')\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```context=dotor(alphas,a)```改为 ```     context=dotor([alphas,a])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a,s_prev):\n",
    "    s_prev=repeator(s_prev)\n",
    "#     concat=concatenator([s_prev,a])\n",
    "    concat=concatenator([a,s_prev])\n",
    "    e=densor1(concat)\n",
    "    energies=densor2(e)\n",
    "    alphas=activator(energies)\n",
    "    context=dotor([alphas,a])\n",
    "    return context\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a=32\n",
    "n_s=64\n",
    "post_activation_LSTM_cell=LSTM(n_s,return_state=True)\n",
    "output_layer=Dense(len(machine_vocab),activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  双向LSTM:  应该是嵌套关系（外层Bidirectional，内层LSTM）\n",
    "- 错误\n",
    "\n",
    "```\n",
    "    a=Bidirectional(n_s,return_state=True)\n",
    "\n",
    "```\n",
    "- 正确\n",
    "\n",
    "```\n",
    "    a=Bidirectional(LSTM(n_s,return_state=True))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### bug\n",
    "\n",
    "```\n",
    "<ipython-input-116-57255807c600> in one_step_attention(a, s_prev)\n",
    "      2     s_prev=RepeatVector(s_prev)\n",
    "      3 #     concat=concatenator([s_prev,a])\n",
    "----> 4     concat=concatenator([a,s_prev])\n",
    "      5     e=densor1(concat)\n",
    "      6     energies=densor2(e)\n",
    "\n",
    "~/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py in __call__(self, inputs, **kwargs)\n",
    "    556                 # Raise exceptions in case the input is not compatible\n",
    "    557                 # with the input_spec specified in the layer constructor.\n",
    "--> 558                 self.assert_input_compatibility(inputs)\n",
    "    559 \n",
    "    560                 # Collect input shapes to build layer.\n",
    "\n",
    "~/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py in assert_input_compatibility(self, inputs)\n",
    "    429                                  'Received type: ' +\n",
    "    430                                  str(type(x)) + '. Full input: ' +\n",
    "--> 431                                  str(inputs) + '. All inputs to the layer '\n",
    "    432                                  'should be tensors.')\n",
    "    433 \n",
    "\n",
    "ValueError: Layer concatenate_3 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.core.RepeatVector'>. Full input: [<tf.Tensor 'bidirectional_14/concat_2:0' shape=(?, ?, 128) dtype=float32>, <keras.layers.core.RepeatVector object at 0x7f65a7af1c18>]. All inputs to the layer should be tensors.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### 【bug】`a=Bidirectional(LSTM(n_s,return_state=True),input_shape=(m,Tx,n_a*2))(X) `中 `return_state=True` 改为 `return_sequence=True`\n",
    "\n",
    "```\n",
    "<ipython-input-103-060c318cc92f> in model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size)\n",
    "      6     c=c0\n",
    "      7 \n",
    "----> 8     a=Bidirectional(LSTM(n_s,return_state=True),input_shape=(m,Tx,n_a*2))(X)\n",
    "      9 \n",
    "     10     outputs=[]\n",
    "     \n",
    "~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in ndim(x)\n",
    "    542     \n",
    "    543     \"\"\"\n",
    "--> 544     dims = x.get_shape()._dims\n",
    "    545     if dims is not None:\n",
    "    546         return len(dims)\n",
    "\n",
    "AttributeError: 'list' object has no attribute 'get_shape'     \n",
    "```\n",
    "\n",
    "#### 【bug】系数比预期 `(None, 30, 64) `翻了一番\n",
    "```\n",
    "bidirectional_1 (Bidirectional)  (None, 30, 128) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx,Ty,n_a,n_s,human_vocab_size,machine_vocab_size):\n",
    "    X=Input(shape=(Tx,human_vocab_size))\n",
    "    s0=Input(shape=(n_s,),name='s0')\n",
    "    c0=Input(shape=(n_s,),name='c0')\n",
    "    s=s0\n",
    "    c=c0\n",
    "    \n",
    "    a=Bidirectional(LSTM(n_a,return_sequences=True),input_shape=(m,Tx,n_a*2))(X)\n",
    "    \n",
    "    outputs=[]\n",
    "    \n",
    "    for t in range(Ty):    \n",
    "        context=one_step_attention(a,s)\n",
    "        s,_,c=post_activation_LSTM_cell(context,initial_state=[s,c])\n",
    "        out=output_layer(s)\n",
    "        outputs.append(out)\n",
    "        \n",
    "    model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=model(Tx,Ty,n_a,n_s,len(human_vocab),len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 30, 64)        17920       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_3[0][0]                     \n",
      "                                                                   lstm_3[1][0]                     \n",
      "                                                                   lstm_3[2][0]                     \n",
      "                                                                   lstm_3[3][0]                     \n",
      "                                                                   lstm_3[4][0]                     \n",
      "                                                                   lstm_3[5][0]                     \n",
      "                                                                   lstm_3[6][0]                     \n",
      "                                                                   lstm_3[7][0]                     \n",
      "                                                                   lstm_3[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 30, 128)       0           bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[0][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[1][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[2][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[3][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[4][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[5][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[6][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[7][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[8][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 30, 10)        1290        concatenate_2[0][0]              \n",
      "                                                                   concatenate_2[1][0]              \n",
      "                                                                   concatenate_2[2][0]              \n",
      "                                                                   concatenate_2[3][0]              \n",
      "                                                                   concatenate_2[4][0]              \n",
      "                                                                   concatenate_2[5][0]              \n",
      "                                                                   concatenate_2[6][0]              \n",
      "                                                                   concatenate_2[7][0]              \n",
      "                                                                   concatenate_2[8][0]              \n",
      "                                                                   concatenate_2[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 30, 1)         11          dense_4[0][0]                    \n",
      "                                                                   dense_4[1][0]                    \n",
      "                                                                   dense_4[2][0]                    \n",
      "                                                                   dense_4[3][0]                    \n",
      "                                                                   dense_4[4][0]                    \n",
      "                                                                   dense_4[5][0]                    \n",
      "                                                                   dense_4[6][0]                    \n",
      "                                                                   dense_4[7][0]                    \n",
      "                                                                   dense_4[8][0]                    \n",
      "                                                                   dense_4[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_5[0][0]                    \n",
      "                                                                   dense_5[1][0]                    \n",
      "                                                                   dense_5[2][0]                    \n",
      "                                                                   dense_5[3][0]                    \n",
      "                                                                   dense_5[4][0]                    \n",
      "                                                                   dense_5[5][0]                    \n",
      "                                                                   dense_5[6][0]                    \n",
      "                                                                   dense_5[7][0]                    \n",
      "                                                                   dense_5[8][0]                    \n",
      "                                                                   dense_5[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1, 64)         0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    [(None, 64), (None, 6 33024       dot_2[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_2[1][0]                      \n",
      "                                                                   lstm_3[0][0]                     \n",
      "                                                                   lstm_3[0][2]                     \n",
      "                                                                   dot_2[2][0]                      \n",
      "                                                                   lstm_3[1][0]                     \n",
      "                                                                   lstm_3[1][2]                     \n",
      "                                                                   dot_2[3][0]                      \n",
      "                                                                   lstm_3[2][0]                     \n",
      "                                                                   lstm_3[2][2]                     \n",
      "                                                                   dot_2[4][0]                      \n",
      "                                                                   lstm_3[3][0]                     \n",
      "                                                                   lstm_3[3][2]                     \n",
      "                                                                   dot_2[5][0]                      \n",
      "                                                                   lstm_3[4][0]                     \n",
      "                                                                   lstm_3[4][2]                     \n",
      "                                                                   dot_2[6][0]                      \n",
      "                                                                   lstm_3[5][0]                     \n",
      "                                                                   lstm_3[5][2]                     \n",
      "                                                                   dot_2[7][0]                      \n",
      "                                                                   lstm_3[6][0]                     \n",
      "                                                                   lstm_3[6][2]                     \n",
      "                                                                   dot_2[8][0]                      \n",
      "                                                                   lstm_3[7][0]                     \n",
      "                                                                   lstm_3[7][2]                     \n",
      "                                                                   dot_2[9][0]                      \n",
      "                                                                   lstm_3[8][0]                     \n",
      "                                                                   lstm_3[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 11)            715         lstm_3[0][0]                     \n",
      "                                                                   lstm_3[1][0]                     \n",
      "                                                                   lstm_3[2][0]                     \n",
      "                                                                   lstm_3[3][0]                     \n",
      "                                                                   lstm_3[4][0]                     \n",
      "                                                                   lstm_3[5][0]                     \n",
      "                                                                   lstm_3[6][0]                     \n",
      "                                                                   lstm_3[7][0]                     \n",
      "                                                                   lstm_3[8][0]                     \n",
      "                                                                   lstm_3[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.005,beta_1=0.9,beta_2=0.999,decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0=np.zeros((m,n_s))\n",
    "c0=np.zeros((m,n_s))\n",
    "outputs=list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 13s - loss: 16.8011 - dense_6_loss_1: 1.1405 - dense_6_loss_2: 0.9764 - dense_6_loss_3: 1.7010 - dense_6_loss_4: 2.6880 - dense_6_loss_5: 0.8111 - dense_6_loss_6: 1.3757 - dense_6_loss_7: 2.7021 - dense_6_loss_8: 1.0605 - dense_6_loss_9: 1.7320 - dense_6_loss_10: 2.6139 - dense_6_acc_1: 0.5447 - dense_6_acc_2: 0.7282 - dense_6_acc_3: 0.3114 - dense_6_acc_4: 0.0740 - dense_6_acc_5: 0.9633 - dense_6_acc_6: 0.3013 - dense_6_acc_7: 0.0509 - dense_6_acc_8: 0.8825 - dense_6_acc_9: 0.2584 - dense_6_acc_10: 0.1031    \n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 11s - loss: 9.0614 - dense_6_loss_1: 0.1354 - dense_6_loss_2: 0.1135 - dense_6_loss_3: 1.0192 - dense_6_loss_4: 2.1661 - dense_6_loss_5: 0.0338 - dense_6_loss_6: 0.3928 - dense_6_loss_7: 1.8798 - dense_6_loss_8: 0.0229 - dense_6_loss_9: 1.1416 - dense_6_loss_10: 2.1563 - dense_6_acc_1: 0.9702 - dense_6_acc_2: 0.9708 - dense_6_acc_3: 0.5209 - dense_6_acc_4: 0.2074 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.8549 - dense_6_acc_7: 0.3385 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.4817 - dense_6_acc_10: 0.2078    \n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 12s - loss: 7.4471 - dense_6_loss_1: 0.0884 - dense_6_loss_2: 0.0733 - dense_6_loss_3: 0.8758 - dense_6_loss_4: 1.9410 - dense_6_loss_5: 0.0181 - dense_6_loss_6: 0.1967 - dense_6_loss_7: 1.4286 - dense_6_loss_8: 0.0140 - dense_6_loss_9: 0.9028 - dense_6_loss_10: 1.9085 - dense_6_acc_1: 0.9764 - dense_6_acc_2: 0.9770 - dense_6_acc_3: 0.6329 - dense_6_acc_4: 0.3072 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9456 - dense_6_acc_7: 0.5286 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.6208 - dense_6_acc_10: 0.2894    \n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 11s - loss: 5.9784 - dense_6_loss_1: 0.0802 - dense_6_loss_2: 0.0630 - dense_6_loss_3: 0.6601 - dense_6_loss_4: 1.4832 - dense_6_loss_5: 0.0149 - dense_6_loss_6: 0.1415 - dense_6_loss_7: 1.1213 - dense_6_loss_8: 0.0146 - dense_6_loss_9: 0.7858 - dense_6_loss_10: 1.6138 - dense_6_acc_1: 0.9778 - dense_6_acc_2: 0.9788 - dense_6_acc_3: 0.7823 - dense_6_acc_4: 0.5202 - dense_6_acc_5: 0.9999 - dense_6_acc_6: 0.9610 - dense_6_acc_7: 0.6209 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.6805 - dense_6_acc_10: 0.4039    \n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 11s - loss: 4.4960 - dense_6_loss_1: 0.0768 - dense_6_loss_2: 0.0566 - dense_6_loss_3: 0.4792 - dense_6_loss_4: 0.9695 - dense_6_loss_5: 0.0121 - dense_6_loss_6: 0.1214 - dense_6_loss_7: 0.8969 - dense_6_loss_8: 0.0146 - dense_6_loss_9: 0.6943 - dense_6_loss_10: 1.1745 - dense_6_acc_1: 0.9796 - dense_6_acc_2: 0.9820 - dense_6_acc_3: 0.8385 - dense_6_acc_4: 0.7408 - dense_6_acc_5: 0.9999 - dense_6_acc_6: 0.9664 - dense_6_acc_7: 0.7112 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.7335 - dense_6_acc_10: 0.6093    \n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 11s - loss: 3.4409 - dense_6_loss_1: 0.0741 - dense_6_loss_2: 0.0538 - dense_6_loss_3: 0.3833 - dense_6_loss_4: 0.6410 - dense_6_loss_5: 0.0097 - dense_6_loss_6: 0.1054 - dense_6_loss_7: 0.7342 - dense_6_loss_8: 0.0124 - dense_6_loss_9: 0.6232 - dense_6_loss_10: 0.8037 - dense_6_acc_1: 0.9786 - dense_6_acc_2: 0.9822 - dense_6_acc_3: 0.8626 - dense_6_acc_4: 0.8552 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9721 - dense_6_acc_7: 0.7779 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.7728 - dense_6_acc_10: 0.7571    \n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 11s - loss: 2.7382 - dense_6_loss_1: 0.0686 - dense_6_loss_2: 0.0489 - dense_6_loss_3: 0.3167 - dense_6_loss_4: 0.4632 - dense_6_loss_5: 0.0077 - dense_6_loss_6: 0.0925 - dense_6_loss_7: 0.6109 - dense_6_loss_8: 0.0105 - dense_6_loss_9: 0.5649 - dense_6_loss_10: 0.5542 - dense_6_acc_1: 0.9790 - dense_6_acc_2: 0.9837 - dense_6_acc_3: 0.8894 - dense_6_acc_4: 0.8999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9745 - dense_6_acc_7: 0.8224 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.7987 - dense_6_acc_10: 0.8379    \n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 11s - loss: 2.2416 - dense_6_loss_1: 0.0635 - dense_6_loss_2: 0.0425 - dense_6_loss_3: 0.2630 - dense_6_loss_4: 0.3589 - dense_6_loss_5: 0.0066 - dense_6_loss_6: 0.0808 - dense_6_loss_7: 0.5050 - dense_6_loss_8: 0.0088 - dense_6_loss_9: 0.5066 - dense_6_loss_10: 0.4060 - dense_6_acc_1: 0.9789 - dense_6_acc_2: 0.9852 - dense_6_acc_3: 0.9094 - dense_6_acc_4: 0.9256 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9776 - dense_6_acc_7: 0.8662 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.8254 - dense_6_acc_10: 0.8871    \n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 12s - loss: 1.9051 - dense_6_loss_1: 0.0585 - dense_6_loss_2: 0.0374 - dense_6_loss_3: 0.2181 - dense_6_loss_4: 0.2918 - dense_6_loss_5: 0.0056 - dense_6_loss_6: 0.0739 - dense_6_loss_7: 0.4381 - dense_6_loss_8: 0.0073 - dense_6_loss_9: 0.4594 - dense_6_loss_10: 0.3150 - dense_6_acc_1: 0.9806 - dense_6_acc_2: 0.9870 - dense_6_acc_3: 0.9325 - dense_6_acc_4: 0.9425 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9785 - dense_6_acc_7: 0.8886 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.8389 - dense_6_acc_10: 0.9160    \n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 11s - loss: 1.6464 - dense_6_loss_1: 0.0544 - dense_6_loss_2: 0.0327 - dense_6_loss_3: 0.1822 - dense_6_loss_4: 0.2409 - dense_6_loss_5: 0.0050 - dense_6_loss_6: 0.0652 - dense_6_loss_7: 0.3835 - dense_6_loss_8: 0.0065 - dense_6_loss_9: 0.4162 - dense_6_loss_10: 0.2596 - dense_6_acc_1: 0.9821 - dense_6_acc_2: 0.9884 - dense_6_acc_3: 0.9490 - dense_6_acc_4: 0.9591 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9818 - dense_6_acc_7: 0.9041 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.8543 - dense_6_acc_10: 0.9344    \n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 11s - loss: 1.4474 - dense_6_loss_1: 0.0509 - dense_6_loss_2: 0.0294 - dense_6_loss_3: 0.1526 - dense_6_loss_4: 0.2021 - dense_6_loss_5: 0.0045 - dense_6_loss_6: 0.0601 - dense_6_loss_7: 0.3449 - dense_6_loss_8: 0.0055 - dense_6_loss_9: 0.3766 - dense_6_loss_10: 0.2208 - dense_6_acc_1: 0.9833 - dense_6_acc_2: 0.9903 - dense_6_acc_3: 0.9625 - dense_6_acc_4: 0.9736 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9819 - dense_6_acc_7: 0.9098 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.8700 - dense_6_acc_10: 0.9426    \n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 12s - loss: 1.2711 - dense_6_loss_1: 0.0480 - dense_6_loss_2: 0.0255 - dense_6_loss_3: 0.1244 - dense_6_loss_4: 0.1716 - dense_6_loss_5: 0.0039 - dense_6_loss_6: 0.0540 - dense_6_loss_7: 0.3090 - dense_6_loss_8: 0.0050 - dense_6_loss_9: 0.3382 - dense_6_loss_10: 0.1915 - dense_6_acc_1: 0.9836 - dense_6_acc_2: 0.9914 - dense_6_acc_3: 0.9739 - dense_6_acc_4: 0.9832 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9857 - dense_6_acc_7: 0.9183 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.8855 - dense_6_acc_10: 0.9523    \n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 12s - loss: 1.1286 - dense_6_loss_1: 0.0444 - dense_6_loss_2: 0.0216 - dense_6_loss_3: 0.1032 - dense_6_loss_4: 0.1447 - dense_6_loss_5: 0.0035 - dense_6_loss_6: 0.0505 - dense_6_loss_7: 0.2821 - dense_6_loss_8: 0.0047 - dense_6_loss_9: 0.3079 - dense_6_loss_10: 0.1659 - dense_6_acc_1: 0.9846 - dense_6_acc_2: 0.9919 - dense_6_acc_3: 0.9826 - dense_6_acc_4: 0.9906 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9868 - dense_6_acc_7: 0.9237 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.8960 - dense_6_acc_10: 0.9594    \n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 11s - loss: 1.0142 - dense_6_loss_1: 0.0425 - dense_6_loss_2: 0.0193 - dense_6_loss_3: 0.0863 - dense_6_loss_4: 0.1225 - dense_6_loss_5: 0.0032 - dense_6_loss_6: 0.0477 - dense_6_loss_7: 0.2590 - dense_6_loss_8: 0.0042 - dense_6_loss_9: 0.2806 - dense_6_loss_10: 0.1488 - dense_6_acc_1: 0.9853 - dense_6_acc_2: 0.9933 - dense_6_acc_3: 0.9879 - dense_6_acc_4: 0.9946 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9857 - dense_6_acc_7: 0.9288 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9060 - dense_6_acc_10: 0.9638    \n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s - loss: 0.9168 - dense_6_loss_1: 0.0387 - dense_6_loss_2: 0.0164 - dense_6_loss_3: 0.0711 - dense_6_loss_4: 0.1063 - dense_6_loss_5: 0.0029 - dense_6_loss_6: 0.0447 - dense_6_loss_7: 0.2373 - dense_6_loss_8: 0.0043 - dense_6_loss_9: 0.2617 - dense_6_loss_10: 0.1335 - dense_6_acc_1: 0.9868 - dense_6_acc_2: 0.9946 - dense_6_acc_3: 0.9918 - dense_6_acc_4: 0.9968 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9871 - dense_6_acc_7: 0.9341 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9134 - dense_6_acc_10: 0.9684    \n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.8282 - dense_6_loss_1: 0.0356 - dense_6_loss_2: 0.0135 - dense_6_loss_3: 0.0592 - dense_6_loss_4: 0.0908 - dense_6_loss_5: 0.0025 - dense_6_loss_6: 0.0420 - dense_6_loss_7: 0.2217 - dense_6_loss_8: 0.0038 - dense_6_loss_9: 0.2402 - dense_6_loss_10: 0.1189 - dense_6_acc_1: 0.9880 - dense_6_acc_2: 0.9958 - dense_6_acc_3: 0.9933 - dense_6_acc_4: 0.9986 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9888 - dense_6_acc_7: 0.9371 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9192 - dense_6_acc_10: 0.9729    \n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.7596 - dense_6_loss_1: 0.0328 - dense_6_loss_2: 0.0116 - dense_6_loss_3: 0.0501 - dense_6_loss_4: 0.0794 - dense_6_loss_5: 0.0023 - dense_6_loss_6: 0.0406 - dense_6_loss_7: 0.2044 - dense_6_loss_8: 0.0037 - dense_6_loss_9: 0.2254 - dense_6_loss_10: 0.1093 - dense_6_acc_1: 0.9895 - dense_6_acc_2: 0.9965 - dense_6_acc_3: 0.9949 - dense_6_acc_4: 0.9990 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9879 - dense_6_acc_7: 0.9419 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9259 - dense_6_acc_10: 0.9747    \n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.6985 - dense_6_loss_1: 0.0304 - dense_6_loss_2: 0.0103 - dense_6_loss_3: 0.0427 - dense_6_loss_4: 0.0696 - dense_6_loss_5: 0.0022 - dense_6_loss_6: 0.0383 - dense_6_loss_7: 0.1936 - dense_6_loss_8: 0.0034 - dense_6_loss_9: 0.2102 - dense_6_loss_10: 0.0979 - dense_6_acc_1: 0.9904 - dense_6_acc_2: 0.9975 - dense_6_acc_3: 0.9961 - dense_6_acc_4: 0.9988 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9887 - dense_6_acc_7: 0.9424 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9317 - dense_6_acc_10: 0.9807    \n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.6439 - dense_6_loss_1: 0.0288 - dense_6_loss_2: 0.0091 - dense_6_loss_3: 0.0376 - dense_6_loss_4: 0.0617 - dense_6_loss_5: 0.0020 - dense_6_loss_6: 0.0362 - dense_6_loss_7: 0.1800 - dense_6_loss_8: 0.0033 - dense_6_loss_9: 0.1951 - dense_6_loss_10: 0.0900 - dense_6_acc_1: 0.9912 - dense_6_acc_2: 0.9976 - dense_6_acc_3: 0.9965 - dense_6_acc_4: 0.9991 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9905 - dense_6_acc_7: 0.9480 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9375 - dense_6_acc_10: 0.9822    \n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.5918 - dense_6_loss_1: 0.0266 - dense_6_loss_2: 0.0078 - dense_6_loss_3: 0.0328 - dense_6_loss_4: 0.0556 - dense_6_loss_5: 0.0018 - dense_6_loss_6: 0.0342 - dense_6_loss_7: 0.1692 - dense_6_loss_8: 0.0032 - dense_6_loss_9: 0.1793 - dense_6_loss_10: 0.0812 - dense_6_acc_1: 0.9925 - dense_6_acc_2: 0.9981 - dense_6_acc_3: 0.9972 - dense_6_acc_4: 0.9994 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9904 - dense_6_acc_7: 0.9503 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9434 - dense_6_acc_10: 0.9860    \n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 12s - loss: 0.5522 - dense_6_loss_1: 0.0252 - dense_6_loss_2: 0.0070 - dense_6_loss_3: 0.0291 - dense_6_loss_4: 0.0498 - dense_6_loss_5: 0.0018 - dense_6_loss_6: 0.0318 - dense_6_loss_7: 0.1579 - dense_6_loss_8: 0.0030 - dense_6_loss_9: 0.1700 - dense_6_loss_10: 0.0766 - dense_6_acc_1: 0.9927 - dense_6_acc_2: 0.9989 - dense_6_acc_3: 0.9980 - dense_6_acc_4: 0.9995 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9909 - dense_6_acc_7: 0.9531 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9489 - dense_6_acc_10: 0.9863    \n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.5126 - dense_6_loss_1: 0.0236 - dense_6_loss_2: 0.0065 - dense_6_loss_3: 0.0263 - dense_6_loss_4: 0.0451 - dense_6_loss_5: 0.0016 - dense_6_loss_6: 0.0306 - dense_6_loss_7: 0.1501 - dense_6_loss_8: 0.0028 - dense_6_loss_9: 0.1558 - dense_6_loss_10: 0.0702 - dense_6_acc_1: 0.9933 - dense_6_acc_2: 0.9992 - dense_6_acc_3: 0.9982 - dense_6_acc_4: 0.9996 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9913 - dense_6_acc_7: 0.9540 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9531 - dense_6_acc_10: 0.9875    \n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 12s - loss: 0.4781 - dense_6_loss_1: 0.0222 - dense_6_loss_2: 0.0059 - dense_6_loss_3: 0.0235 - dense_6_loss_4: 0.0413 - dense_6_loss_5: 0.0015 - dense_6_loss_6: 0.0290 - dense_6_loss_7: 0.1425 - dense_6_loss_8: 0.0028 - dense_6_loss_9: 0.1450 - dense_6_loss_10: 0.0644 - dense_6_acc_1: 0.9947 - dense_6_acc_2: 0.9995 - dense_6_acc_3: 0.9985 - dense_6_acc_4: 0.9998 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9914 - dense_6_acc_7: 0.9573 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9589 - dense_6_acc_10: 0.9893    \n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.4509 - dense_6_loss_1: 0.0209 - dense_6_loss_2: 0.0052 - dense_6_loss_3: 0.0215 - dense_6_loss_4: 0.0380 - dense_6_loss_5: 0.0014 - dense_6_loss_6: 0.0288 - dense_6_loss_7: 0.1353 - dense_6_loss_8: 0.0026 - dense_6_loss_9: 0.1371 - dense_6_loss_10: 0.0602 - dense_6_acc_1: 0.9952 - dense_6_acc_2: 0.9995 - dense_6_acc_3: 0.9990 - dense_6_acc_4: 0.9997 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9911 - dense_6_acc_7: 0.9585 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9607 - dense_6_acc_10: 0.9905    \n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.4194 - dense_6_loss_1: 0.0198 - dense_6_loss_2: 0.0049 - dense_6_loss_3: 0.0198 - dense_6_loss_4: 0.0351 - dense_6_loss_5: 0.0013 - dense_6_loss_6: 0.0266 - dense_6_loss_7: 0.1276 - dense_6_loss_8: 0.0026 - dense_6_loss_9: 0.1259 - dense_6_loss_10: 0.0559 - dense_6_acc_1: 0.9953 - dense_6_acc_2: 0.9995 - dense_6_acc_3: 0.9989 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9922 - dense_6_acc_7: 0.9611 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9659 - dense_6_acc_10: 0.9912    \n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.3928 - dense_6_loss_1: 0.0189 - dense_6_loss_2: 0.0045 - dense_6_loss_3: 0.0180 - dense_6_loss_4: 0.0325 - dense_6_loss_5: 0.0012 - dense_6_loss_6: 0.0254 - dense_6_loss_7: 0.1203 - dense_6_loss_8: 0.0025 - dense_6_loss_9: 0.1175 - dense_6_loss_10: 0.0519 - dense_6_acc_1: 0.9963 - dense_6_acc_2: 0.9996 - dense_6_acc_3: 0.9993 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9918 - dense_6_acc_7: 0.9644 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9685 - dense_6_acc_10: 0.9913    \n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.3691 - dense_6_loss_1: 0.0180 - dense_6_loss_2: 0.0041 - dense_6_loss_3: 0.0165 - dense_6_loss_4: 0.0304 - dense_6_loss_5: 0.0012 - dense_6_loss_6: 0.0240 - dense_6_loss_7: 0.1149 - dense_6_loss_8: 0.0024 - dense_6_loss_9: 0.1100 - dense_6_loss_10: 0.0476 - dense_6_acc_1: 0.9963 - dense_6_acc_2: 0.9996 - dense_6_acc_3: 0.9995 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9930 - dense_6_acc_7: 0.9663 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9726 - dense_6_acc_10: 0.9924    \n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.3472 - dense_6_loss_1: 0.0171 - dense_6_loss_2: 0.0038 - dense_6_loss_3: 0.0155 - dense_6_loss_4: 0.0283 - dense_6_loss_5: 0.0011 - dense_6_loss_6: 0.0226 - dense_6_loss_7: 0.1086 - dense_6_loss_8: 0.0023 - dense_6_loss_9: 0.1028 - dense_6_loss_10: 0.0452 - dense_6_acc_1: 0.9970 - dense_6_acc_2: 0.9996 - dense_6_acc_3: 0.9995 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9938 - dense_6_acc_7: 0.9679 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9751 - dense_6_acc_10: 0.9933    \n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s - loss: 0.3298 - dense_6_loss_1: 0.0161 - dense_6_loss_2: 0.0035 - dense_6_loss_3: 0.0145 - dense_6_loss_4: 0.0265 - dense_6_loss_5: 0.0010 - dense_6_loss_6: 0.0227 - dense_6_loss_7: 0.1041 - dense_6_loss_8: 0.0023 - dense_6_loss_9: 0.0966 - dense_6_loss_10: 0.0425 - dense_6_acc_1: 0.9972 - dense_6_acc_2: 0.9996 - dense_6_acc_3: 0.9996 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9928 - dense_6_acc_7: 0.9696 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9782 - dense_6_acc_10: 0.9935    \n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 12s - loss: 0.3075 - dense_6_loss_1: 0.0154 - dense_6_loss_2: 0.0032 - dense_6_loss_3: 0.0133 - dense_6_loss_4: 0.0253 - dense_6_loss_5: 9.9209e-04 - dense_6_loss_6: 0.0199 - dense_6_loss_7: 0.0986 - dense_6_loss_8: 0.0022 - dense_6_loss_9: 0.0888 - dense_6_loss_10: 0.0399 - dense_6_acc_1: 0.9972 - dense_6_acc_2: 0.9997 - dense_6_acc_3: 0.9995 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9939 - dense_6_acc_7: 0.9714 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9823 - dense_6_acc_10: 0.9945    \n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.2915 - dense_6_loss_1: 0.0146 - dense_6_loss_2: 0.0031 - dense_6_loss_3: 0.0126 - dense_6_loss_4: 0.0235 - dense_6_loss_5: 9.6307e-04 - dense_6_loss_6: 0.0198 - dense_6_loss_7: 0.0943 - dense_6_loss_8: 0.0022 - dense_6_loss_9: 0.0830 - dense_6_loss_10: 0.0375 - dense_6_acc_1: 0.9974 - dense_6_acc_2: 0.9996 - dense_6_acc_3: 0.9996 - dense_6_acc_4: 0.9999 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9939 - dense_6_acc_7: 0.9741 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9835 - dense_6_acc_10: 0.9948    \n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.2723 - dense_6_loss_1: 0.0139 - dense_6_loss_2: 0.0029 - dense_6_loss_3: 0.0118 - dense_6_loss_4: 0.0222 - dense_6_loss_5: 9.1846e-04 - dense_6_loss_6: 0.0182 - dense_6_loss_7: 0.0888 - dense_6_loss_8: 0.0021 - dense_6_loss_9: 0.0773 - dense_6_loss_10: 0.0343 - dense_6_acc_1: 0.9980 - dense_6_acc_2: 0.9997 - dense_6_acc_3: 0.9996 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9941 - dense_6_acc_7: 0.9753 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9859 - dense_6_acc_10: 0.9952    \n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.2571 - dense_6_loss_1: 0.0133 - dense_6_loss_2: 0.0027 - dense_6_loss_3: 0.0112 - dense_6_loss_4: 0.0209 - dense_6_loss_5: 8.8442e-04 - dense_6_loss_6: 0.0169 - dense_6_loss_7: 0.0845 - dense_6_loss_8: 0.0021 - dense_6_loss_9: 0.0716 - dense_6_loss_10: 0.0329 - dense_6_acc_1: 0.9976 - dense_6_acc_2: 0.9997 - dense_6_acc_3: 0.9996 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9953 - dense_6_acc_7: 0.9778 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9889 - dense_6_acc_10: 0.9950    \n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.2441 - dense_6_loss_1: 0.0128 - dense_6_loss_2: 0.0025 - dense_6_loss_3: 0.0104 - dense_6_loss_4: 0.0200 - dense_6_loss_5: 8.1640e-04 - dense_6_loss_6: 0.0159 - dense_6_loss_7: 0.0808 - dense_6_loss_8: 0.0020 - dense_6_loss_9: 0.0684 - dense_6_loss_10: 0.0304 - dense_6_acc_1: 0.9978 - dense_6_acc_2: 0.9997 - dense_6_acc_3: 0.9998 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9955 - dense_6_acc_7: 0.9787 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9894 - dense_6_acc_10: 0.9960    \n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.2321 - dense_6_loss_1: 0.0123 - dense_6_loss_2: 0.0025 - dense_6_loss_3: 0.0099 - dense_6_loss_4: 0.0188 - dense_6_loss_5: 7.8234e-04 - dense_6_loss_6: 0.0158 - dense_6_loss_7: 0.0773 - dense_6_loss_8: 0.0019 - dense_6_loss_9: 0.0641 - dense_6_loss_10: 0.0289 - dense_6_acc_1: 0.9980 - dense_6_acc_2: 0.9997 - dense_6_acc_3: 0.9997 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9951 - dense_6_acc_7: 0.9795 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9900 - dense_6_acc_10: 0.9957    \n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 12s - loss: 0.2208 - dense_6_loss_1: 0.0118 - dense_6_loss_2: 0.0023 - dense_6_loss_3: 0.0095 - dense_6_loss_4: 0.0179 - dense_6_loss_5: 7.7208e-04 - dense_6_loss_6: 0.0156 - dense_6_loss_7: 0.0733 - dense_6_loss_8: 0.0019 - dense_6_loss_9: 0.0604 - dense_6_loss_10: 0.0274 - dense_6_acc_1: 0.9982 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9997 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9959 - dense_6_acc_7: 0.9810 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9911 - dense_6_acc_10: 0.9961    \n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.2088 - dense_6_loss_1: 0.0112 - dense_6_loss_2: 0.0021 - dense_6_loss_3: 0.0090 - dense_6_loss_4: 0.0170 - dense_6_loss_5: 7.1700e-04 - dense_6_loss_6: 0.0136 - dense_6_loss_7: 0.0702 - dense_6_loss_8: 0.0018 - dense_6_loss_9: 0.0575 - dense_6_loss_10: 0.0256 - dense_6_acc_1: 0.9982 - dense_6_acc_2: 0.9997 - dense_6_acc_3: 0.9998 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9969 - dense_6_acc_7: 0.9817 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9919 - dense_6_acc_10: 0.9965    \n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1971 - dense_6_loss_1: 0.0108 - dense_6_loss_2: 0.0021 - dense_6_loss_3: 0.0084 - dense_6_loss_4: 0.0162 - dense_6_loss_5: 7.0627e-04 - dense_6_loss_6: 0.0133 - dense_6_loss_7: 0.0674 - dense_6_loss_8: 0.0018 - dense_6_loss_9: 0.0524 - dense_6_loss_10: 0.0240 - dense_6_acc_1: 0.9985 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9997 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9962 - dense_6_acc_7: 0.9828 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9927 - dense_6_acc_10: 0.9973    \n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1878 - dense_6_loss_1: 0.0103 - dense_6_loss_2: 0.0020 - dense_6_loss_3: 0.0081 - dense_6_loss_4: 0.0155 - dense_6_loss_5: 6.7531e-04 - dense_6_loss_6: 0.0125 - dense_6_loss_7: 0.0638 - dense_6_loss_8: 0.0017 - dense_6_loss_9: 0.0501 - dense_6_loss_10: 0.0231 - dense_6_acc_1: 0.9984 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9998 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9968 - dense_6_acc_7: 0.9839 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9929 - dense_6_acc_10: 0.9970    \n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1805 - dense_6_loss_1: 0.0099 - dense_6_loss_2: 0.0018 - dense_6_loss_3: 0.0078 - dense_6_loss_4: 0.0146 - dense_6_loss_5: 6.5594e-04 - dense_6_loss_6: 0.0129 - dense_6_loss_7: 0.0614 - dense_6_loss_8: 0.0016 - dense_6_loss_9: 0.0476 - dense_6_loss_10: 0.0222 - dense_6_acc_1: 0.9985 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9998 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9965 - dense_6_acc_7: 0.9841 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9939 - dense_6_acc_10: 0.9973    \n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1697 - dense_6_loss_1: 0.0095 - dense_6_loss_2: 0.0017 - dense_6_loss_3: 0.0073 - dense_6_loss_4: 0.0142 - dense_6_loss_5: 6.1554e-04 - dense_6_loss_6: 0.0113 - dense_6_loss_7: 0.0589 - dense_6_loss_8: 0.0016 - dense_6_loss_9: 0.0443 - dense_6_loss_10: 0.0204 - dense_6_acc_1: 0.9986 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9999 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9975 - dense_6_acc_7: 0.9854 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9941 - dense_6_acc_10: 0.9979    \n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 12s - loss: 0.1625 - dense_6_loss_1: 0.0092 - dense_6_loss_2: 0.0017 - dense_6_loss_3: 0.0070 - dense_6_loss_4: 0.0134 - dense_6_loss_5: 5.9589e-04 - dense_6_loss_6: 0.0112 - dense_6_loss_7: 0.0566 - dense_6_loss_8: 0.0016 - dense_6_loss_9: 0.0416 - dense_6_loss_10: 0.0196 - dense_6_acc_1: 0.9989 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9999 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9968 - dense_6_acc_7: 0.9864 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9948 - dense_6_acc_10: 0.9983    \n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s - loss: 0.1549 - dense_6_loss_1: 0.0088 - dense_6_loss_2: 0.0016 - dense_6_loss_3: 0.0067 - dense_6_loss_4: 0.0129 - dense_6_loss_5: 5.7192e-04 - dense_6_loss_6: 0.0105 - dense_6_loss_7: 0.0538 - dense_6_loss_8: 0.0015 - dense_6_loss_9: 0.0397 - dense_6_loss_10: 0.0189 - dense_6_acc_1: 0.9989 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9999 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9975 - dense_6_acc_7: 0.9878 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9949 - dense_6_acc_10: 0.9983    \n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1482 - dense_6_loss_1: 0.0085 - dense_6_loss_2: 0.0015 - dense_6_loss_3: 0.0065 - dense_6_loss_4: 0.0123 - dense_6_loss_5: 5.5718e-04 - dense_6_loss_6: 0.0103 - dense_6_loss_7: 0.0516 - dense_6_loss_8: 0.0014 - dense_6_loss_9: 0.0378 - dense_6_loss_10: 0.0179 - dense_6_acc_1: 0.9988 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 0.9999 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9974 - dense_6_acc_7: 0.9884 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9953 - dense_6_acc_10: 0.9982    \n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1421 - dense_6_loss_1: 0.0081 - dense_6_loss_2: 0.0014 - dense_6_loss_3: 0.0062 - dense_6_loss_4: 0.0117 - dense_6_loss_5: 5.3593e-04 - dense_6_loss_6: 0.0097 - dense_6_loss_7: 0.0504 - dense_6_loss_8: 0.0013 - dense_6_loss_9: 0.0357 - dense_6_loss_10: 0.0169 - dense_6_acc_1: 0.9989 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 1.0000 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9978 - dense_6_acc_7: 0.9886 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9955 - dense_6_acc_10: 0.9985    \n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1360 - dense_6_loss_1: 0.0078 - dense_6_loss_2: 0.0014 - dense_6_loss_3: 0.0059 - dense_6_loss_4: 0.0114 - dense_6_loss_5: 5.1244e-04 - dense_6_loss_6: 0.0093 - dense_6_loss_7: 0.0480 - dense_6_loss_8: 0.0013 - dense_6_loss_9: 0.0341 - dense_6_loss_10: 0.0163 - dense_6_acc_1: 0.9991 - dense_6_acc_2: 0.9998 - dense_6_acc_3: 1.0000 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9982 - dense_6_acc_7: 0.9891 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9957 - dense_6_acc_10: 0.9983    \n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1304 - dense_6_loss_1: 0.0076 - dense_6_loss_2: 0.0013 - dense_6_loss_3: 0.0058 - dense_6_loss_4: 0.0109 - dense_6_loss_5: 4.5705e-04 - dense_6_loss_6: 0.0090 - dense_6_loss_7: 0.0462 - dense_6_loss_8: 0.0013 - dense_6_loss_9: 0.0324 - dense_6_loss_10: 0.0154 - dense_6_acc_1: 0.9990 - dense_6_acc_2: 0.9999 - dense_6_acc_3: 1.0000 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9984 - dense_6_acc_7: 0.9906 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9958 - dense_6_acc_10: 0.9985    \n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 12s - loss: 0.1250 - dense_6_loss_1: 0.0074 - dense_6_loss_2: 0.0013 - dense_6_loss_3: 0.0054 - dense_6_loss_4: 0.0104 - dense_6_loss_5: 4.6467e-04 - dense_6_loss_6: 0.0089 - dense_6_loss_7: 0.0444 - dense_6_loss_8: 0.0012 - dense_6_loss_9: 0.0305 - dense_6_loss_10: 0.0150 - dense_6_acc_1: 0.9993 - dense_6_acc_2: 0.9999 - dense_6_acc_3: 1.0000 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9981 - dense_6_acc_7: 0.9908 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9960 - dense_6_acc_10: 0.9987    \n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1195 - dense_6_loss_1: 0.0069 - dense_6_loss_2: 0.0012 - dense_6_loss_3: 0.0052 - dense_6_loss_4: 0.0100 - dense_6_loss_5: 4.5620e-04 - dense_6_loss_6: 0.0084 - dense_6_loss_7: 0.0428 - dense_6_loss_8: 0.0012 - dense_6_loss_9: 0.0293 - dense_6_loss_10: 0.0140 - dense_6_acc_1: 0.9994 - dense_6_acc_2: 0.9999 - dense_6_acc_3: 1.0000 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9981 - dense_6_acc_7: 0.9911 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9961 - dense_6_acc_10: 0.9987    \n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 11s - loss: 0.1155 - dense_6_loss_1: 0.0067 - dense_6_loss_2: 0.0012 - dense_6_loss_3: 0.0050 - dense_6_loss_4: 0.0096 - dense_6_loss_5: 4.4238e-04 - dense_6_loss_6: 0.0084 - dense_6_loss_7: 0.0415 - dense_6_loss_8: 0.0011 - dense_6_loss_9: 0.0280 - dense_6_loss_10: 0.0137 - dense_6_acc_1: 0.9994 - dense_6_acc_2: 0.9999 - dense_6_acc_3: 1.0000 - dense_6_acc_4: 1.0000 - dense_6_acc_5: 1.0000 - dense_6_acc_6: 0.9983 - dense_6_acc_7: 0.9918 - dense_6_acc_8: 1.0000 - dense_6_acc_9: 0.9961 - dense_6_acc_10: 0.9987    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8f47fc6a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-04-04\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-05-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('models/model.h5')\n",
    "\n",
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 30, 64)        17920       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_3[0][0]                     \n",
      "                                                                   lstm_3[1][0]                     \n",
      "                                                                   lstm_3[2][0]                     \n",
      "                                                                   lstm_3[3][0]                     \n",
      "                                                                   lstm_3[4][0]                     \n",
      "                                                                   lstm_3[5][0]                     \n",
      "                                                                   lstm_3[6][0]                     \n",
      "                                                                   lstm_3[7][0]                     \n",
      "                                                                   lstm_3[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 30, 128)       0           bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[0][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[1][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[2][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[3][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[4][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[5][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[6][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[7][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[8][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_2[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 30, 10)        1290        concatenate_2[0][0]              \n",
      "                                                                   concatenate_2[1][0]              \n",
      "                                                                   concatenate_2[2][0]              \n",
      "                                                                   concatenate_2[3][0]              \n",
      "                                                                   concatenate_2[4][0]              \n",
      "                                                                   concatenate_2[5][0]              \n",
      "                                                                   concatenate_2[6][0]              \n",
      "                                                                   concatenate_2[7][0]              \n",
      "                                                                   concatenate_2[8][0]              \n",
      "                                                                   concatenate_2[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 30, 1)         11          dense_4[0][0]                    \n",
      "                                                                   dense_4[1][0]                    \n",
      "                                                                   dense_4[2][0]                    \n",
      "                                                                   dense_4[3][0]                    \n",
      "                                                                   dense_4[4][0]                    \n",
      "                                                                   dense_4[5][0]                    \n",
      "                                                                   dense_4[6][0]                    \n",
      "                                                                   dense_4[7][0]                    \n",
      "                                                                   dense_4[8][0]                    \n",
      "                                                                   dense_4[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_5[0][0]                    \n",
      "                                                                   dense_5[1][0]                    \n",
      "                                                                   dense_5[2][0]                    \n",
      "                                                                   dense_5[3][0]                    \n",
      "                                                                   dense_5[4][0]                    \n",
      "                                                                   dense_5[5][0]                    \n",
      "                                                                   dense_5[6][0]                    \n",
      "                                                                   dense_5[7][0]                    \n",
      "                                                                   dense_5[8][0]                    \n",
      "                                                                   dense_5[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1, 64)         0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    [(None, 64), (None, 6 33024       dot_2[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_2[1][0]                      \n",
      "                                                                   lstm_3[0][0]                     \n",
      "                                                                   lstm_3[0][2]                     \n",
      "                                                                   dot_2[2][0]                      \n",
      "                                                                   lstm_3[1][0]                     \n",
      "                                                                   lstm_3[1][2]                     \n",
      "                                                                   dot_2[3][0]                      \n",
      "                                                                   lstm_3[2][0]                     \n",
      "                                                                   lstm_3[2][2]                     \n",
      "                                                                   dot_2[4][0]                      \n",
      "                                                                   lstm_3[3][0]                     \n",
      "                                                                   lstm_3[3][2]                     \n",
      "                                                                   dot_2[5][0]                      \n",
      "                                                                   lstm_3[4][0]                     \n",
      "                                                                   lstm_3[4][2]                     \n",
      "                                                                   dot_2[6][0]                      \n",
      "                                                                   lstm_3[5][0]                     \n",
      "                                                                   lstm_3[5][2]                     \n",
      "                                                                   dot_2[7][0]                      \n",
      "                                                                   lstm_3[6][0]                     \n",
      "                                                                   lstm_3[6][2]                     \n",
      "                                                                   dot_2[8][0]                      \n",
      "                                                                   lstm_3[7][0]                     \n",
      "                                                                   lstm_3[7][2]                     \n",
      "                                                                   dot_2[9][0]                      \n",
      "                                                                   lstm_3[8][0]                     \n",
      "                                                                   lstm_3[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 11)            715         lstm_3[0][0]                     \n",
      "                                                                   lstm_3[1][0]                     \n",
      "                                                                   lstm_3[2][0]                     \n",
      "                                                                   lstm_3[3][0]                     \n",
      "                                                                   lstm_3[4][0]                     \n",
      "                                                                   lstm_3[5][0]                     \n",
      "                                                                   lstm_3[6][0]                     \n",
      "                                                                   lstm_3[7][0]                     \n",
      "                                                                   lstm_3[8][0]                     \n",
      "                                                                   lstm_3[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda265b8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8a8705c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

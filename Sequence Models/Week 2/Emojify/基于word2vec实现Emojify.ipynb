{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å¦‚ä½•å°†è®­ç»ƒå¥½çš„word2vecåŠ è½½åˆ°ç¨‹åºä¸­ï¼Œæ–¹ä¾¿ä½¿ç”¨\n",
    "    - è¾“å…¥ï¼šå­˜å‚¨å€¼æ˜¯æ¯ä¸ªè¯çš„å‘é‡\n",
    "    - è¾“å‡ºï¼šword2vec, word2index,index2word\n",
    "- ç®€å•çš„åˆ†ç±»è®­ç»ƒï¼šavg è¯å‘é‡è¿›è¡Œåˆ¤æ–­\n",
    "    - é¢„å¤„ç†ï¼šå…¨éƒ¨è½¬æ¢ä¸ºå°å†™\n",
    "    - æ„å»ºç®€å•çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "- åŸºäºLSTMè¿›è¡Œè®­ç»ƒï¼Œé¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 åŠ è½½ word matrix\n",
    "- è¯»å–æ–‡ä»¶çš„æ–¹æ³•å¾ˆæœ‰ç”¨ï¼Œéœ€è¦æŒæ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# pip install emoji\n",
    "head -n 2 data/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    # ç”Ÿæˆ word2vec, word2index,index2word\n",
    "    word_dict={}\n",
    "    words=set()\n",
    "    with open(glove_file,'r') as f:\n",
    "        for l in f:\n",
    "            line=l.strip().split()\n",
    "            cur_word=line[0]\n",
    "            words.add(cur_word)\n",
    "            word_dict[cur_word]=np.array(line[1:],dtype=np.float32)\n",
    "        index2word={k:v for k,v in enumerate(words)}\n",
    "        word2index={ v:k for k,v in enumerate(words)}\n",
    "    return word2index,index2word,word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index,index_to_word,word_to_vec_map=read_glove_vecs(\"./data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 120586\n",
      "the 79898th word in the vocabulary is shurugwi\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 79898\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ç”Ÿæˆå¥å­çš„è¯å‘é‡\n",
    "- lower  splitåè¿›è¡Œè½¬æ¢å³å¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence,word_to_vec_map):\n",
    "    s=sentence.lower().split()\n",
    "    avg_list=[]\n",
    "    for w in s:\n",
    "        if w in s:\n",
    "            avg_list.append(word_to_vec_map[w])\n",
    "    avg_vec=np.vstack(avg_list).mean(axis=0)  \n",
    "    return avg_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00800501  0.56370836 -0.50427336  0.258865    0.5513111   0.03104983\n",
      " -0.21013719  0.16893934 -0.09590267  0.141784   -0.15708967  0.18525869\n",
      "  0.6495786   0.3837112   0.21102166  0.11301666  0.02613968  0.26037768\n",
      "  0.05820667 -0.01578168 -0.12078834 -0.02471267  0.41284552  0.5152061\n",
      "  0.38756165 -0.89866096 -0.535145    0.33501163  0.68806934 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775336  0.75078493  0.10282584  0.348925\n",
      " -0.27262834  0.66767997 -0.10706166 -0.283635    0.5958012   0.28747332\n",
      " -0.33666348  0.23393816  0.34349182  0.178405    0.1166155  -0.076433\n",
      "  0.14454171  0.09808666]\n"
     ]
    }
   ],
   "source": [
    "s=\"Morrocan couscous is my favorite dish\"\n",
    "\n",
    "print(sentence_to_avg(s,word_to_vec_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3 ç”Ÿæˆç®€å•çš„åˆ†ç±»æ¨¡å‹\n",
    "- åªè¦å‘Šè¯‰æˆ‘ è¾“å…¥æ˜¯ä»€ä¹ˆï¼Œè¾“å‡ºæ˜¯ä»€ä¹ˆ \n",
    "    - å¦‚æœæœ‰å¤šä¸ªç»“æœï¼Œé€‰æ‹©å“ªä¸ªå‘¢ï¼Ÿ\n",
    "- è¯å‘é‡è½¬æ¢ä¸º\n",
    "- åˆ†ç±»è½¬æ¢ä¸ºone-hot\n",
    "- æ„å»ºå•å±‚softmaxæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French macaroon is so tasty,4,,\r\n",
      "work is horrible,3,,\r\n",
      "I am upset,3,, [3]\r\n",
      "throw the ball,1,, [2]\r\n",
      "Good joke,2,,\r\n",
      "what is your favorite baseball game,1,,\r\n",
      "I cooked meat,4,,\r\n",
      "stop messing around,3,,\r\n",
      "I want chinese food,4,,\r\n",
      "Let us go play baseball,1,,\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head  ./data/emojify_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):    \n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            l=l.strip('\"').split(',')\n",
    "            X.append(l[0])\n",
    "            Y.append(l[1])        \n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y,dtype=int)\n",
    "    return X,Y\n",
    "X_train,Y_train=read_csv(\"./data/emojify_data.csv\")\n",
    "X_test,Y_test=read_csv(\"./data/tesss.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "maxLen=len(max(X_train,key=len).split())\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[str(label)],use_aliases=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am upset ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "index=2\n",
    "print(X_train[index],label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¤ï¸\n",
      "âš¾\n",
      "ğŸ˜„\n",
      "ğŸ˜\n",
      "ğŸ´\n"
     ]
    }
   ],
   "source": [
    "for k,v in emoji_dictionary.items():\n",
    "    print(emoji.emojize(v,use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(y,C):\n",
    "    return np.eye(C)[y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 183)\n",
      "(5, 56)\n"
     ]
    }
   ],
   "source": [
    "Y_oh_train=convert_to_one_hot(Y_train,C=5).transpose()\n",
    "Y_oh_test=convert_to_one_hot(Y_test,C=5).transpose()\n",
    "print(Y_oh_train.shape)\n",
    "print(Y_oh_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 is converted into one hot [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[:,index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­å»ºmodel \n",
    "- ä½¿ç”¨tensorflow å’Œ keras åˆ†åˆ«å†™ä¸€é\n",
    "\n",
    "- Xçš„shapeæ˜¯ (n_x,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "num_iterations=2\n",
    "\n",
    "\n",
    "X_train_vector=np.transpose(np.vstack([sentence_to_avg(i,word_to_vec_map)  for i in X_train]))\n",
    "X_test_vector=np.transpose(np.vstack([sentence_to_avg(i,word_to_vec_map)  for i in X_test]))\n",
    "\n",
    "n_x=X_train_vector.shape[0]\n",
    "n_y=5\n",
    "\n",
    "# ç”»å›¾\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(shape=(n_x,None),name='X',dtype=tf.float32)\n",
    "Y=tf.placeholder(shape=(n_y,None),name='Y',dtype=tf.float32)\n",
    "W=tf.get_variable(shape=(n_y,n_x),name='W',dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "Z=tf.matmul(W,X)\n",
    "\n",
    "loss=tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=Z)\n",
    "opt=tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# å¼€å§‹è¿è¡Œ\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i_iter in range(num_iterations):\n",
    "        _,l=sess.run([opt,loss],feed_dict={X:X_train_vector,Y:Y_oh_train})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 183)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 183)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_oh_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(5, ?) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_1:0' shape=(50, ?) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'W:0' shape=(5, 50) dtype=float32_ref>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

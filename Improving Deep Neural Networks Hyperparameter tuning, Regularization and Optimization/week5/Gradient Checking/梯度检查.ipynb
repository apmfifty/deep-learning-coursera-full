{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度检查\n",
    "\n",
    "- 梯度检查提供了除算例外的另外一种验算方法\n",
    "- 运算量较大的原因：需要对每一个参数计算一次数值梯度，然后比较整体的误差\n",
    "\n",
    "\n",
    "<img src=\"my_file/gradient_check.jpg\" style=\"width:1000px;height:800px;transform:rotate(90deg);\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 一维情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(x,theta):\n",
    "    \"\"\"\n",
    "    正向传播\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x 固定值\n",
    "    theta 需要求梯度的参数\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    J : 计算值            \n",
    "    \"\"\"\n",
    "    J=x*theta\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J= 8\n"
     ]
    }
   ],
   "source": [
    "x,theta=2,4\n",
    "J=forward_propagation(x,theta)\n",
    "print(\"J=\",J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(x,theta):\n",
    "    \"\"\"\n",
    "    反向传播\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : \n",
    "    theta\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dtheta : \n",
    "  \n",
    "    \"\"\"\n",
    "    dtheta=x\n",
    "    return dtheta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta= 2\n"
     ]
    }
   ],
   "source": [
    "x,theta=2,4\n",
    "dtheta=backward_propagation(x,theta)\n",
    "print(\"dtheta=\",dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x,theta,epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    梯度检查\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : \n",
    "    theta\n",
    "    epsilon\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    difference : \n",
    "    \"\"\"\n",
    "    \n",
    "    theta_plus=theta+epsilon\n",
    "    theta_minus=theta-epsilon    \n",
    "    J_plus=forward_propagation(x,theta_plus)\n",
    "    J_minus=forward_propagation(x,theta_minus)\n",
    "    gradapprox=(J_plus-J_minus)/(2*epsilon)\n",
    "\n",
    "\n",
    "    grad=backward_propagation(x,theta)\n",
    "    \n",
    "    numerator=np.linalg.norm(grad-gradapprox)\n",
    "    denominator=(np.linalg.norm(grad)+np.linalg.norm(gradapprox))\n",
    "    difference=numerator/denominator\n",
    "\n",
    "    if difference<1e-7:\n",
    "        print(\"The gradient is correct\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong\")        \n",
    "    return difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct\n",
      "difference = 0.0\n"
     ]
    }
   ],
   "source": [
    "x, theta = 5., 10.\n",
    "difference = gradient_check(x, theta,epsilon=1)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N维情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 遇到错误\n",
    "- 【bug1】\n",
    "```\n",
    "There is a mistake in the backward propagation! difference = 0.974463960867091\n",
    "```\n",
    "    - 原因 损失函数写错了，忘记了 log\n",
    "    `    cost=-(np.dot(Y.ravel(),A3.ravel())+np.dot((1-Y).ravel(),(1-A3).ravel()))/m`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation_n(X,Y,parameters):\n",
    "    \"\"\"\n",
    "    正向传播\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : \n",
    "    Y \n",
    "    parameters :参数 W b\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost\n",
    "    cache :         \n",
    "    \"\"\"\n",
    "    m=X.shape[1]\n",
    "    W1=parameters['W1']\n",
    "    b1=parameters['b1']\n",
    "    W2=parameters['W2']\n",
    "    b2=parameters['b2']\n",
    "    W3=parameters['W3']\n",
    "    b3=parameters['b3']    \n",
    "    \n",
    "    \n",
    "    Z1=np.matmul(W1,X)+b1\n",
    "    A1=relu(Z1)\n",
    "    Z2=np.matmul(W2,A1)+b2\n",
    "    A2=relu(Z2)\n",
    "    Z3=np.matmul(W3,A2)+b3\n",
    "    A3=sigmoid(Z3)\n",
    "    \n",
    "    cost=-(np.dot(Y.ravel(),np.log(A3.ravel()))+np.dot((1-Y).ravel(),np.log((1-A3).ravel())))/m\n",
    "    cache=(Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3)\n",
    "    return cost,cache\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"my_file/back_propagation_gradient.png\" style=\"width:400px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 遇到bug\n",
    "\n",
    "- element-wise 操作用 np.multiply而不是 np.matmul\n",
    "\n",
    "```\n",
    "<ipython-input-26-daacff1163cd> in backward_propagation(X, Y, cache)\n",
    "     12 \n",
    "     13     dA1=np.matmul(W2.T,dZ2)\n",
    "---> 14     dZ1=np.matmul(dA1,A1>0)\n",
    "     15     dW1=np.matmul(dZ1,X.T)/m\n",
    "     16     db1=np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "\n",
    "ValueError: shapes (5,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation_n(X,Y,cache):\n",
    "    m=X.shape[1]\n",
    "    Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3=cache\n",
    "    dZ3=A3-Y\n",
    "    dW3=np.matmul(dZ3,A2.T)/m\n",
    "    db3=np.sum(dZ3,axis=1,keepdims=True)/m\n",
    "    \n",
    "    dA2=np.matmul(W3.T,dZ3)\n",
    "    dZ2=np.multiply(dA2,A2>0)\n",
    "    dW2=np.matmul(dZ2,A1.T)/m\n",
    "    db2=np.sum(dZ2,axis=1,keepdims=True)/m    \n",
    "    \n",
    "    dA1=np.matmul(W2.T,dZ2)\n",
    "    dZ1=np.multiply(dA1,A1>0)\n",
    "    dW1=np.matmul(dZ1,X.T)/m\n",
    "    db1=np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "     \n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dict 与 vector之间进行相互转换：原因就是需要遍历每一个vector元素，然后对比梯度差异\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[:20].reshape((5,4))\n",
    "    parameters[\"b1\"] = theta[20:25].reshape((5,1))\n",
    "    parameters[\"W2\"] = theta[25:40].reshape((3,5))\n",
    "    parameters[\"b2\"] = theta[40:43].reshape((3,1))\n",
    "    parameters[\"W3\"] = theta[43:46].reshape((1,3))\n",
    "    parameters[\"b3\"] = theta[46:47].reshape((1,1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters,gradients,X,Y,epsilon=1e-7):\n",
    "    parameters_values,_=dictionary_to_vector(parameters)\n",
    "    grad=gradients_to_vector(gradients)\n",
    "\n",
    "    parameters_values,_=dictionary_to_vector(parameters)\n",
    "    grad=gradients_to_vector(gradients)\n",
    "    num_parameters=parameters_values.size\n",
    "    J_plus=np.zeros((num_parameters,1))\n",
    "    J_minus=np.zeros((num_parameters,1))\n",
    "    gradapprox=np.zeros((num_parameters,1))\n",
    "\n",
    "    for i in range(num_parameters):\n",
    "        theta_plus=parameters_values.copy()\n",
    "        theta_plus[i][0]+=epsilon\n",
    "        J_plus,_= forward_propagation_n(X,Y,vector_to_dictionary(theta_plus))\n",
    "\n",
    "        theta_minus=parameters_values.copy()\n",
    "        theta_minus[i][0]-=epsilon\n",
    "        J_minus,_= forward_propagation_n(X,Y,vector_to_dictionary(theta_minus))\n",
    "\n",
    "\n",
    "        gradapprox[i][0]=(J_plus-J_minus)/(2*epsilon)\n",
    "\n",
    "\n",
    "    numerator=np.linalg.norm(grad-gradapprox)\n",
    "    denominator=(np.linalg.norm(grad)+np.linalg.norm(gradapprox))\n",
    "    difference=numerator/denominator\n",
    "\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check_n_test_case(): \n",
    "    np.random.seed(1)\n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "\n",
    "    \n",
    "    return x, y, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.1909939369700076e-07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "cost,cache=forward_propagation_n(X,Y,parameters)\n",
    "gradients=backward_propagation_n(X,Y,cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
